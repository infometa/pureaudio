以下是按照阶段 0、1、2、3 划分的可执行方案，结合已有的 `soundflow` 项目代码结构（以 `DeepFilterCapture` 为核心），每一阶段都给出具体要做的任务，以便后续用 AI 辅助生成代码实现。

---

## 阶段 0：打通基础链路并统一采样率

**目标：** 在不改动核心算法的前提下，确保采集‑降噪‑播放流程稳定运行，统一内部采样率，并为后续扩展打好基础。

1. **统一采样率与声道：**

   * 在创建 `DeepFilterCapture` 时，始终读取 DeepFilterNet 模型的采样率（一般是 48 kHz）和帧长，强制把输入/输出流的采样率配置为该值；对非 48 kHz 输入，继续使用已有的 Rubato 重采样逻辑。
   * 若输入设备是多声道，用已有的 `mean` 或选择主通道进行下混；输出设备多声道则复制输出进行上混。

2. **清理与文档化参数：**

   * 梳理现有控制参数（如 `atten_lim`、`df_mix`、AGC 目标电平等），列成表格，并定义默认值。
   * 在代码中将每个参数的范围和含义用注释说明，便于后续调整。

3. **录音功能验证：**

   * 使用 `RecordingState` 在不同环境下（安静、办公室、嘈杂）录制“原始音频”“降噪输出”“完整处理输出”三组文件。
   * 手工听感、使用 Audacity 查看波形/频谱，初步检查 pipeline 的稳定性和噪声抑制效果。

4. **预定义 EQ 和后处理 preset：**

   * 定义至少两个 EQ preset，例如 “自然”（轻微补偿）和 “广播”（提升 presence、高频），用于后续调试。

此阶段完成后，你应该有一套能够稳定运行的流程，输出音频没有明显爆音或失真，可开始进行性能评估。

---

## 阶段 1：输入设备校准与自动增益控制（AGC）

**目标：** 减少不同麦克风或录音设备对信号幅度的影响，通过校准和 AGC 使处理链条的输入电平保持一致。

1. **实现校准向导：**

   * 在前端（终端或 GUI）加入一个“设备校准”流程，提示用户对着麦克风朗读 10 秒固定文本。
   * 在代码中捕获这段录音，计算语音段的平均 RMS 电平和噪声段的平均 RMS。
   * 根据结果输出建议：如语音 RMS 小于 -30 dBFS，提示调高设备增益；如经常达到 -3 dBFS，则提示调低。

2. **配置 AGC 默认参数：**

   * 使用 `AutoGainControl`，设定合理的目标电平（如 -20 dBFS），最大增益（如 +15 dB），最大衰减（如 -15 dB），以及攻击/释放时间。
   * 开发接口允许通过 `ControlMessage` 调整这些参数，以便 GUI 调整。

3. **头间隙（headroom）与限幅保护：**

   * 现有代码使用 `headroom_gain` 和 `BusLimiter` 保护 DF 输出避免削顶。可通过校准步骤自动设置 headroom，例如根据校准测得的峰值动态调整。

4. **测试与验证：**

   * 在至少三种不同麦克风上重复校准流程，观察 AGC 输出的增益曲线是否稳定。
   * 录制并比较校准前后的音频，确保音量一致且不引入明显噪声。

完成阶段 1 后，系统应能自动调整录音电平，不同设备下的噪声抑制效果更一致。

---

## 阶段 2：环境自适应降噪

**目标：** 让算法根据环境噪声特性动态调整降噪强度和后处理参数，从而兼顾嘈杂场景和安静场景的人声自然度。

1. **加入 VAD 和噪声特征分析：**

   * 在 `get_worker_fn` 中插入一段代码，在处理循环开始处利用能量阈值和过零率实现简单的语音活动检测 (Voice Activity Detection)。
   * 当检测到纯噪声段时，计算以下特征：

     * **噪声能量**（平均功率或 RMS）：反映噪声大小。
     * **谱平坦度**（Spectral Flatness）：判断噪声是宽带还是有突出频率成分。
     * **谱重心**（Spectral Centroid）：噪声重心的频率位置。
   * 可以利用 [librosa](https://librosa.org/) 或自写 FFT 计算这些指标。

2. **实现规则式环境分类器：**

   * 根据特征划分环境，例如：

     * **安静**：能量低、谱平坦度适中。
     * **稳定机械噪声**（空调/电脑风扇）：谱平坦度低、能量中等。
     * **非平稳人声噪声**（餐厅/地铁）：谱平坦度高、能量大且波动明显。
     * **极端嘈杂**：能量极高，SNR 很低。
   * 为每个环境定义一套参数：

     * 调整 `atten_lim`、`min_thresh_db` 等 DeepFilterNet 参数；嘈杂环境使用较大衰减，安静环境保持较低抑制。
     * 调整 `df_mix`（干/湿比）以适当保留原始音色。
     * 调整高通截止频率（例如安静场景 60 Hz、嘈杂场景 100 Hz）。
     * 切换 EQ preset 和启用/禁用瞬态、饱和器等模块。

3. **平滑过渡：**

   * 在控制参数发生变化时，不要一帧切换；设计一个滑动窗口，按 1–2 秒的时间常数线性过渡参数，以避免音色突变。

4. **扩展：轻量环境识别模型（可选）：**

   * 收集一小批带标签的噪声段数据（标签为会议室/办公室/咖啡厅/地铁等）。
   * 训练一个小型 CNN/GRU 模型输入噪声的对数频谱，输出环境类别；在应用时使用该模型代替手动规则，提升准确性。

5. **测试与调优：**

   * 在典型环境下录制噪声和语音，验证环境分类是否正确。
   * 调整各 preset 参数，平衡噪声抑制和音色自然度。

阶段 2 完成后，系统应能根据环境自动切换降噪策略，减少极端噪声时的人声损伤。

---

## 阶段 3：音色还原与风格 preset

**目标：** 在有效降噪的同时尽量保持人声质感，通过后处理模块补偿音色。

1. **完善动态均衡器 (DynamicEq)：**

   * 基于分析结果设计至少三套 preset：

     * **自然**：温和补偿，2–4 kHz 略提升，4–8 kHz 轻微 high‑shelf，动态压缩比低。
     * **广播主持**：presence 区域提升明显，高频拉亮，压缩比中等，适合视频/播客场景。
     * **嘈杂环境**：高频削弱以避免残余噪声，低频合理截断。
   * 利用现有的 `EqControl::SetPreset` 接口实现 preset 切换；在 GUI 中提供选择。

2. **瞬态整形与饱和器调优：**

   * 为瞬态整形 (`TransientShaper`) 设计默认攻防增益 (Attack/Sustain Gain) 和干湿比，突出语音起音，抑制尾音拖尾。
   * 为饱和器 (`Saturation`) 设定合理的驱动和补偿增益，使声音更加饱满但不失真；在安静环境可适当关闭。

3. **轻量后滤 AI 模型（可选）：**

   * 将 DeepFilterNet 输出与原始干净语音频谱作为训练数据，训练一个 U‑Net 或 GRU 网络输出频域掩膜或残差，控制增益在 ±3 dB 内。
   * 部署时在 `outframe` 处理完 DF 输出后调用该网络修正频谱，使音色更接近原始干净语音。

4. **干湿混合与限幅优化：**

   * 调整 `df_mix` 的默认值，根据环境动态调整混合比例，嘈杂环境保持 1.0，安静环境可降低至 0.8，提升自然度。
   * 优化 `BusLimiter` 和 `apply_final_limiter` 的参数，保证长时间听感舒适。

5. **盲听与主观调优：**

   * 邀请数名听众在不同 preset 下盲听评估，收集反馈，适当调整 EQ 滤波器频率、增益和动态参数。

完成阶段 3 后，系统应能提供多种风格的音色还原，在不同应用场景（会议、播客、直播）中获得较好的主观听感。

---

## 开发建议

1. 每一阶段都建议先编写单元测试或简单脚本，验证新增功能是否生效，如校准向导是否正确调整 AGC，环境分类器的输出是否合理。
2. 在代码层面，尽量将新功能封装为独立模块或结构体（例如 `NoiseAnalyzer`、`EnvironmentPresetManager`），以便维护。
3. 使用 Git 分支管理不同阶段的开发，阶段完成后合并回主分支，保持版本可回退。
4. 完善日志输出和错误处理，方便调试；必要时使用如 `serialport-rs` 等库输出调试信息。

---

按照上述方案逐步实施，可以系统性地提升你基于 DeepFilterNet 的音频处理项目，使其兼顾多环境、自适应、多设备适配和音色还原，为后续利用 AI 进一步强化提供坚实基础。
